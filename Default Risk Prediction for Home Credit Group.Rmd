---
title: "Default Risk Prediction for Home Credit Group"
author: "Dongzhe Zhang"
date: "10/10/2019"
output:
  html_document:
    df_print: paged
---

#### This is my first machine learning project

### Set up
```{r message=FALSE}
library(tidyverse)
library(ggthemes) 
library(randomForest) 
library(gbm)
library(MASS)
library(glmnet)
library(rpart) 
library(rpart.plot)
library(caret)
theme_set(theme_economist())
```


```{r message=F}
original_dataset <- read_csv("dataset/application.csv")
```

### Data Cleaning
1. A lot of columns contain missing values. 
Instead of replacing them with the median, we would take the columns that have more than 40% NA's out.
```{r}
# calculate the missing values proportion for each variable
na_prop <- colSums(is.na(original_dataset)) / nrow(original_dataset)
# Find the variables that have over 40% missing values
na_40 <- sort(na_prop[na_prop > 0.4], decreasing = TRUE)
# remove these columns
original_dataset <- original_dataset[ ,!names(original_dataset) %in% names(na_40)]
```

2. There are columns that we don't understanding the meaning of such as `FLAG_DOCUMENT` and `SOCIAL_CIRCLE`. Since we cannot find any additional information about them, we decided to remove these variables as well.
```{r}
original_dataset = original_dataset[-grep("FLAG_DOCUMENT",colnames(original_dataset))]
original_dataset = original_dataset[-grep("SOCIAL_CIRCLE",colnames(original_dataset))]
```

We also decided to remove any column that contains `CITY` in them since there are other columns that define the applicant's `REGION` and some variables that describe the characteristics of the `REGION`, using `CITY` again seems redundant and overlapping.
```{r}
original_dataset = original_dataset[-grep("CITY", colnames(original_dataset))]
```

Because of the same reason, we decided to remove some of the columns that contain `AMT_REQ_CREDIT_BUREAU`, only keep `AMT_REQ_CREDIT_BUREAU_WEEK` represent short-term count of credit requirements and `AMT_REQ_CREDIT_BUREAU_YEAR` as long_term count of credit requirements.

```{r}
names = c("AMT_REQ_CREDIT_BUREAU_HOUR", "AMT_REQ_CREDIT_BUREAU_DAY", "AMT_REQ_CREDIT_BUREAU_MON", "AMT_REQ_CREDIT_BUREAU_QRT")
original_dataset = original_dataset[,-which(names(original_dataset) %in% names) ]
```


3. `DAYS_EMPLOYED` represents the days that the applicant is employed until the application date, which whould be all negative in this dataset. Therefore, the value `365243` in `DAYS_EMPLOYED` column seems unreasonable and we would replace it with 0.
```{r}
original_dataset$DAYS_EMPLOYED[which(original_dataset$DAYS_EMPLOYED == 365243)] <- 0
```

For better understanding of the data, we also need to convert `DAYS_EMPLOYED`, `DAYS_BIRTH`, `DAYS_PUBLISH` and `DAYS_REGISTRATION`, which are presented as negative in the dataset, to positive number in years.
```{r}
original_dataset$DAYS_EMPLOYED[which(original_dataset$DAYS_EMPLOYED == 365243)] <- 0
original_dataset$DAYS_EMPLOYED = abs(original_dataset$DAYS_EMPLOYED)/365 %>% floor()
original_dataset$DAYS_BIRTH = abs(original_dataset$DAYS_BIRTH)/365 %>% floor()
original_dataset$DAYS_ID_PUBLISH = abs(original_dataset$DAYS_ID_PUBLISH)/365 %>% floor()
original_dataset$DAYS_REGISTRATION = abs(original_dataset$DAYS_REGISTRATION)/365 %>% floor()
```

4. There are some false entries in `AMT_REQ_CREDIT_BUREAU_WEEK` and `AMT_REQ_CREDIT_BUREAU_YEAR`, so we removed all observations with false entries.
```{r}
original_dataset<-original_dataset%>% filter((is.na(AMT_REQ_CREDIT_BUREAU_WEEK)&is.na(AMT_REQ_CREDIT_BUREAU_YEAR))|
                               (AMT_REQ_CREDIT_BUREAU_WEEK <=AMT_REQ_CREDIT_BUREAU_YEAR))
```

Remove XNA in `CODE_GENDER`
```{r}
original_dataset <- original_dataset %>% filter(CODE_GENDER != "XNA")
```

Set XNA in `ORGANIZATION_TYPE` to `Not_provide`
```{r}
original_dataset[original_dataset=="XNA"] <- "Not Provided"
```


5. With columns that are left with less than 40% NA's in them, we replaced those NA's with the median of the variable.
```{r}
ext2_median <- median(original_dataset$EXT_SOURCE_2, na.rm = TRUE)
ext3_median <- median(original_dataset$EXT_SOURCE_3, na.rm = TRUE)
 
original_dataset<- original_dataset%>% replace_na(list(EXT_SOURCE_2 = ext2_median, 
                           EXT_SOURCE_3 = ext3_median))

phonechange_median <- median(original_dataset$DAYS_LAST_PHONE_CHANGE, na.rm = TRUE)
original_dataset<- original_dataset%>% replace_na(list(DAYS_LAST_PHONE_CHANGE = phonechange_median))

week_median <- median(original_dataset$AMT_REQ_CREDIT_BUREAU_WEEK, na.rm = TRUE)
year_median <- median(original_dataset$AMT_REQ_CREDIT_BUREAU_YEAR, na.rm = TRUE)
 
original_dataset<- original_dataset%>% replace_na(list(AMT_REQ_CREDIT_BUREAU_WEEK = week_median, 
                           AMT_REQ_CREDIT_BUREAU_YEAR = year_median))
```

We replaced NA in `Annuity` to 0
```{r}
original_dataset$AMT_ANNUITY[is.na(original_dataset$AMT_ANNUITY)] <- 0
```

We replace NA in `Good Price` column to 0
```{r}
original_dataset$AMT_GOODS_PRICE[is.na(original_dataset$AMT_GOODS_PRICE)] <- 0
```

We also removed unkown family status observations in the data.
```{r}
unknow_status = which(is.na(original_dataset$CNT_FAM_MEMBERS))
original_dataset = original_dataset[-unknow_status,]
```

We then set other NA's as "not_provided" level
```{r}
original_dataset[is.na(original_dataset)] <- "Not Provided"
```


And last but not least, we factored all the columns in the dataset.
```{r}
original_dataset <- as.data.frame(unclass(original_dataset))
```


### Exploratory Data Analysis
Before we go ahead to build different models for our dataset, we need to take a look at the data that we have.
```{r}
ggplot(original_dataset)+
  geom_bar(aes(x=TARGET,col=TARGET))+
  scale_x_discrete(limits=c(0,1))
```
From this graph we can see that the proportion of default(1) and not default(0) are highly different. Therefore, when we separate the dataset into train and test datasets, we need to make sure that the there are enough default(1) in both train and test datasets. Therefore, we would randomly select 20% from 0 and 1 as the test dataset.

```{r}
set.seed(7)
dd_default = original_dataset %>% filter(TARGET==1)
dd_default %>% 
  mutate(TRAIN = sample(c(0,1),nrow(dd_default),replace=T,prob=c(0.2,0.8))) ->dd_default

dd_not_default = original_dataset %>% filter(TARGET == 0)
dd_not_default %>% 
  mutate(TRAIN = sample(c(0,1),nrow(dd_not_default),replace=T,prob=c(0.2,0.8))) ->dd_not_default

dd_clean = rbind(dd_default,dd_not_default)

application_train = dd_clean[which(dd_clean$TRAIN==1),]
application_test = dd_clean[which(dd_clean$TRAIN==0),]

```

In addition to the above dataset, we also created another dataset that has converted all the categorical variables into dummy variables in the datset. Since LASSO and Ridge would not automatically convert categorical variables, we created this dataset for LASSO and Ridge.
\
```{r}
dmy <- dummyVars(formula = ~., data = application_train, fullRank = TRUE)
dummy_train <- data.frame(predict(dmy, newdata = application_train))

dmy <- dummyVars(formula = ~., data = application_test, fullRank = TRUE)
dummy_test <- data.frame(predict(dmy, newdata = application_test))

```

In order to save time, We decided to take $\frac{1}{10}$ of `application_train` to be `subset_train`, and used it to find out the optimized forward, backwoard selection and tree-based model.   

```{r}
set.seed(7)
subset_train <- application_train[sample(1:nrow(application_train),nrow(application_train)/10),]
dummy_subset_train <- dummy_train[sample(1:nrow(application_train),nrow(application_train)/10),]
```
 
### Linear Regression
Before we jump into Lasso and Ridge, a simple linear regression is needed for a overall understanding of the data.
```{r, warning = FALSE}
model_lm <- lm(TARGET~ . -SK_ID_CURR -TRAIN,data=application_train)

# Compute training MSE
yhat_lm_train <- predict(model_lm, application_train)
mse_lm_train <- mean((application_train$TARGET - yhat_lm_train)^2)

# Compute test MSE
yhat_lm_test <- predict(model_lm, application_test)
mse_lm_test <- mean((application_test$TARGET- yhat_lm_test)^2)

summary(model_lm)
print(paste("MSE of training dataset is", signif(mse_lm_train,4 )))
print(paste("MSE of testing dataset is", signif(mse_lm_test,4 )))
```

We select out the top 10 predictors both negative or positive affect the default probability.
```{r}

topcof <- sort(model_lm$coefficients, decreasing = TRUE)
topcof[1:10]

leastcof <- sort(model_lm$coefficients)
leastcof[1:10]

```

### Lasso & Ridge

```{r}
c_names <- colnames(dummy_train)
c_names <- c_names[!c_names %in% c("SK_ID_CURR", "TARGET")] 

loopformula <- "TARGET ~ NAME_CONTRACT_TYPE.Revolving.loans"

for (name in c_names[2:length(c_names)]) {
  loopformula <- paste0(loopformula, "+", name, sep = "")
}

f_all <- as.formula(loopformula) 
```

Set x_test, x_train, y_test, x_train
```{r}
x1_train <- model.matrix(f_all, dummy_train)[ , -1]
y1_train <- dummy_train$TARGET

x1_test <- model.matrix(f_all, dummy_test)[ ,-1] 
y1_test <- dummy_test$TARGET
```

```{r}
## run lasso regression  
fit_lasso <- cv.glmnet(x1_train, y1_train, alpha = 1, nfolds = 10)

# compute MSE train
yhat_lasso_train <- predict(fit_lasso, x1_train, s = fit_lasso$lambda.min)
mse_lasso_train <- mean((y1_train - yhat_lasso_train)^2)

# compute MSE test
yhat_lasso_test <- predict(fit_lasso, x1_test, s = fit_lasso$lambda.min)
mse_lasso_test <- mean((y1_test - yhat_lasso_test)^2) 

#find out the variables with values after lasso regression
temp <- coef(fit_lasso)
temp2 <- coef(fit_lasso)
temp2 <- as.data.frame(summary(temp2))
cbind ( as.vector(temp@Dimnames[[1]]) [temp2$i], temp2$x)

## run ridge regression 
fit_Ridge <- cv.glmnet(x1_train, y1_train, alpha = 0, nfolds = 10)

# compute MSE train
yhat_Ridge_train <- predict(fit_Ridge, x1_train, s = fit_Ridge$lambda.min)
mse_Ridge_train <- mean((y1_train - yhat_Ridge_train)^2)

# compute MSE test
yhat_Ridge_test <- predict(fit_Ridge, x1_test, s = fit_Ridge$lambda.min)
mse_Ridge_test <- mean((y1_test - yhat_Ridge_test)^2)

#output the coefficients of ridge regression
coef(fit_Ridge)


```


### Forward Selection
After the lasso and ridge regression, we also want to see the best predictors through forward and backward selection.
First, we would start with the simplest model, which only contains the intercept.

```{r, eval=FALSE, echo = TRUE}

null <- lm(TARGET ~ 1, data = dummy_subset_train)
full <- lm(TARGET ~ . -SK_ID_CURR -TRAIN, data = dummy_subset_train)

forward.lm <- step(null, scope=list(lower=null, upper=full), 
                   direction="forward")

summary(forward.lm)
#In order to save time and notebook sapce and make the outcome more clear, We didn't run the code again for knitting. Instead, we paste the final output as below.(for both forward and backward selection)
```

Call:
lm(formula = TARGET ~ EXT_SOURCE_2 + EXT_SOURCE_3 + CODE_GENDER.M + 
    NAME_EDUCATION_TYPE.Higher.education + DAYS_BIRTH + FLAG_OWN_CAR.Y + 
    NAME_CONTRACT_TYPE.Revolving.loans + NAME_INCOME_TYPE.Working + 
    DAYS_EMPLOYED + DAYS_ID_PUBLISH + OCCUPATION_TYPE.High.skill.tech.staff + 
    OCCUPATION_TYPE.Low.skill.Laborers + FLAG_WORK_PHONE + NAME_INCOME_TYPE.Commercial.associate + 
    REGION_RATING_CLIENT + ORGANIZATION_TYPE.Construction + NAME_EDUCATION_TYPE.Incomplete.higher + 
    NAME_HOUSING_TYPE.With.parents + WEEKDAY_APPR_PROCESS_START.SUNDAY + 
    NAME_TYPE_SUITE.Unaccompanied + AMT_ANNUITY + AMT_GOODS_PRICE + 
    AMT_CREDIT + ORGANIZATION_TYPE.Realtor + AMT_REQ_CREDIT_BUREAU_WEEK + 
    WEEKDAY_APPR_PROCESS_START.MONDAY + ORGANIZATION_TYPE.Industry..type.13 + 
    OCCUPATION_TYPE.Cooking.staff + NAME_TYPE_SUITE.Other_B + 
    ORGANIZATION_TYPE.Mobile + ORGANIZATION_TYPE.School + FLAG_PHONE + 
    ORGANIZATION_TYPE.Security + ORGANIZATION_TYPE.Transport..type.3 + 
    ORGANIZATION_TYPE.Bank + DAYS_LAST_PHONE_CHANGE + ORGANIZATION_TYPE.Housing + 
    ORGANIZATION_TYPE.Emergency + ORGANIZATION_TYPE.Industry..type.7 + 
    LIVE_REGION_NOT_WORK_REGION + OCCUPATION_TYPE.Laborers + 
    ORGANIZATION_TYPE.Cleaning + ORGANIZATION_TYPE.Transport..type.2 + 
    NAME_FAMILY_STATUS.Single...not.married, data = dummy_subset_train)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.41207 -0.11806 -0.06565 -0.01302  1.08794 

Coefficients:
                                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)                              2.791e-01  1.600e-02  17.448  < 2e-16 ***
EXT_SOURCE_2                            -1.848e-01  9.779e-03 -18.897  < 2e-16 ***
EXT_SOURCE_3                            -2.059e-01  1.003e-02 -20.541  < 2e-16 ***
CODE_GENDER.M                            3.109e-02  4.094e-03   7.595 3.19e-14 ***
NAME_EDUCATION_TYPE.Higher.education    -2.650e-02  4.270e-03  -6.205 5.56e-10 ***
DAYS_BIRTH                              -6.135e-04  1.851e-04  -3.314 0.000922 ***
FLAG_OWN_CAR.Y                          -2.468e-02  3.953e-03  -6.243 4.37e-10 ***
NAME_CONTRACT_TYPE.Revolving.loans      -1.998e-02  6.181e-03  -3.233 0.001226 ** 
NAME_INCOME_TYPE.Working                 2.059e-02  5.240e-03   3.930 8.52e-05 ***
DAYS_EMPLOYED                           -1.425e-03  2.922e-04  -4.876 1.09e-06 ***
DAYS_ID_PUBLISH                         -1.386e-03  4.348e-04  -3.187 0.001440 ** 
OCCUPATION_TYPE.High.skill.tech.staff   -2.981e-02  9.039e-03  -3.299 0.000973 ***
OCCUPATION_TYPE.Low.skill.Laborers       7.073e-02  2.117e-02   3.341 0.000836 ***
FLAG_WORK_PHONE                          2.222e-02  4.678e-03   4.750 2.04e-06 ***
NAME_INCOME_TYPE.Commercial.associate    1.280e-02  5.837e-03   2.194 0.028257 *  
REGION_RATING_CLIENT                     1.178e-02  3.629e-03   3.247 0.001169 ** 
ORGANIZATION_TYPE.Construction           2.693e-02  1.146e-02   2.349 0.018819 *  
NAME_EDUCATION_TYPE.Incomplete.higher   -2.338e-02  9.817e-03  -2.381 0.017262 *  
NAME_HOUSING_TYPE.With.parents           2.007e-02  8.089e-03   2.481 0.013120 *  
WEEKDAY_APPR_PROCESS_START.SUNDAY       -1.991e-02  7.738e-03  -2.573 0.010096 *  
NAME_TYPE_SUITE.Unaccompanied            1.260e-02  4.482e-03   2.812 0.004928 ** 
AMT_ANNUITY                              7.920e-07  1.927e-07   4.111 3.96e-05 ***
AMT_GOODS_PRICE                         -2.060e-07  3.008e-08  -6.849 7.60e-12 ***
AMT_CREDIT                               1.710e-07  2.731e-08   6.261 3.90e-10 ***
ORGANIZATION_TYPE.Realtor                1.169e-01  4.886e-02   2.392 0.016758 *  
AMT_REQ_CREDIT_BUREAU_WEEK              -2.747e-02  1.143e-02  -2.403 0.016282 *  
WEEKDAY_APPR_PROCESS_START.MONDAY       -1.053e-02  4.680e-03  -2.250 0.024479 *  
ORGANIZATION_TYPE.Industry..type.13      2.658e-01  1.195e-01   2.223 0.026204 *  
OCCUPATION_TYPE.Cooking.staff            3.033e-02  1.256e-02   2.416 0.015719 *  
NAME_TYPE_SUITE.Other_B                  4.968e-02  2.426e-02   2.048 0.040582 *  
ORGANIZATION_TYPE.Mobile                -1.132e-01  5.463e-02  -2.073 0.038210 *  
ORGANIZATION_TYPE.School                -2.123e-02  1.039e-02  -2.043 0.041046 *  
FLAG_PHONE                              -7.783e-03  4.032e-03  -1.930 0.053595 .  
ORGANIZATION_TYPE.Security              -2.893e-02  1.663e-02  -1.739 0.081999 .  
ORGANIZATION_TYPE.Transport..type.3      5.085e-02  2.774e-02   1.833 0.066768 .  
ORGANIZATION_TYPE.Bank                  -3.393e-02  1.949e-02  -1.741 0.081768 .  
DAYS_LAST_PHONE_CHANGE                   3.768e-06  2.141e-06   1.760 0.078379 .  
ORGANIZATION_TYPE.Housing               -3.085e-02  1.727e-02  -1.787 0.073950 .  
ORGANIZATION_TYPE.Emergency             -6.476e-02  3.908e-02  -1.657 0.097519 .  
ORGANIZATION_TYPE.Industry..type.7      -4.438e-02  2.609e-02  -1.701 0.089016 .  
LIVE_REGION_NOT_WORK_REGION             -1.508e-02  9.015e-03  -1.673 0.094360 .  
OCCUPATION_TYPE.Laborers                 7.505e-03  4.908e-03   1.529 0.126204    
ORGANIZATION_TYPE.Cleaning               9.569e-02  5.977e-02   1.601 0.109436    
ORGANIZATION_TYPE.Transport..type.2      3.007e-02  2.076e-02   1.449 0.147480    
NAME_FAMILY_STATUS.Single...not.married  7.167e-03  4.974e-03   1.441 0.149600    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.267 on 24402 degrees of freedom
Multiple R-squared:  0.06976,	Adjusted R-squared:  0.06808 
F-statistic: 41.59 on 44 and 24402 DF,  p-value: < 2.2e-16



```{r, eval=FALSE, echo= TRUE}
fwd_names <- names(forward.lm$coefficients)
fwd_loop <- "TARGET ~ "

for (name in fwd_names[2: length(fwd_names)]) {
  fwd_loop <- paste0(fwd_loop, "+", name, sep = "")
}

fwd_all <- as.formula(fwd_loop)
fwd <- lm(fwd_all, data = dummy_train)
```

Compute training and test MSE

```{r, eval=FALSE, echo= TRUE}
# Compute training MSE
yhat_fwd_train <- predict(fwd)
mse_fwd_train <- mean((dummy_train$TARGET- yhat_fwd_train)^2)

# Compute test MSE
yhat_fwd_test <- predict(fwd, dummy_test)
mse_fwd_test <- mean((application_test$TARGET- yhat_fwd_test)^2)

print(paste("MSE of training dataset is", signif(mse_fwd_train,4 )))
print(paste("MSE of testing dataset is", signif(mse_fwd_test,4 )))
```

We reuse the MSE from our previous process.
```{r}

mse_fwd_train = 0.06986
mse_fwd_test = 0.06988

print(paste("MSE of training dataset is", signif(mse_fwd_train,4 )))
print(paste("MSE of testing dataset is", signif(mse_fwd_test,4 )))
```

### Backward Selection

```{r, eval=FALSE, echo=TRUE}
backward.lm <- stepAIC(full, scope=list(lower=null, upper=full), 
                    direction="backward")
```

Step:  AIC=-64521.53
TARGET ~ NAME_CONTRACT_TYPE.Revolving.loans + CODE_GENDER.M + 
    FLAG_OWN_CAR.Y + AMT_CREDIT + AMT_ANNUITY + AMT_GOODS_PRICE + 
    NAME_TYPE_SUITE.Family + NAME_TYPE_SUITE.Other_B + NAME_TYPE_SUITE.Spouse..partner + 
    NAME_INCOME_TYPE.Commercial.associate + NAME_INCOME_TYPE.State.servant + 
    NAME_EDUCATION_TYPE.Lower.secondary + NAME_EDUCATION_TYPE.Secondary...secondary.special + 
    NAME_FAMILY_STATUS.Widow + NAME_HOUSING_TYPE.House...apartment + 
    DAYS_BIRTH + DAYS_EMPLOYED + DAYS_ID_PUBLISH + FLAG_WORK_PHONE + 
    FLAG_PHONE + OCCUPATION_TYPE.Cooking.staff + OCCUPATION_TYPE.High.skill.tech.staff + 
    OCCUPATION_TYPE.Laborers + OCCUPATION_TYPE.Low.skill.Laborers + 
    REGION_RATING_CLIENT + WEEKDAY_APPR_PROCESS_START.MONDAY + 
    WEEKDAY_APPR_PROCESS_START.SUNDAY + LIVE_REGION_NOT_WORK_REGION + 
    ORGANIZATION_TYPE.Business.Entity.Type.3 + ORGANIZATION_TYPE.Cleaning + 
    ORGANIZATION_TYPE.Construction + ORGANIZATION_TYPE.Industry..type.1 + 
    ORGANIZATION_TYPE.Industry..type.13 + ORGANIZATION_TYPE.Insurance + 
    ORGANIZATION_TYPE.Legal.Services + ORGANIZATION_TYPE.Medicine + 
    ORGANIZATION_TYPE.Mobile + ORGANIZATION_TYPE.Other + ORGANIZATION_TYPE.Realtor + 
    ORGANIZATION_TYPE.Self.employed + ORGANIZATION_TYPE.Transport..type.2 + 
    ORGANIZATION_TYPE.Transport..type.3 + ORGANIZATION_TYPE.Transport..type.4 + 
    EXT_SOURCE_2 + EXT_SOURCE_3 + DAYS_LAST_PHONE_CHANGE + AMT_REQ_CREDIT_BUREAU_WEEK

                                                    Df Sum of Sq    RSS    AIC
<none>                                                           1739.0 -64522
- ORGANIZATION_TYPE.Insurance                        1    0.1432 1739.2 -64522
- ORGANIZATION_TYPE.Medicine                         1    0.1440 1739.2 -64522
- ORGANIZATION_TYPE.Legal.Services                   1    0.1522 1739.2 -64521
- NAME_FAMILY_STATUS.Widow                           1    0.1541 1739.2 -64521
- NAME_EDUCATION_TYPE.Lower.secondary                1    0.1747 1739.2 -64521
- ORGANIZATION_TYPE.Transport..type.4                1    0.1836 1739.2 -64521
- NAME_TYPE_SUITE.Other_B                            1    0.1845 1739.2 -64521
- NAME_TYPE_SUITE.Spouse..partner                    1    0.1962 1739.2 -64521
- ORGANIZATION_TYPE.Industry..type.1                 1    0.2001 1739.2 -64521
- LIVE_REGION_NOT_WORK_REGION                        1    0.2050 1739.3 -64521
- DAYS_LAST_PHONE_CHANGE                             1    0.2260 1739.3 -64520
- ORGANIZATION_TYPE.Mobile                           1    0.2264 1739.3 -64520
- OCCUPATION_TYPE.Laborers                           1    0.2336 1739.3 -64520
- ORGANIZATION_TYPE.Cleaning                         1    0.2423 1739.3 -64520
- NAME_INCOME_TYPE.Commercial.associate              1    0.2425 1739.3 -64520
- FLAG_PHONE                                         1    0.2955 1739.3 -64519
- ORGANIZATION_TYPE.Transport..type.2                1    0.2978 1739.3 -64519
- NAME_HOUSING_TYPE.House...apartment                1    0.3338 1739.4 -64519
- ORGANIZATION_TYPE.Other                            1    0.3468 1739.4 -64519
- WEEKDAY_APPR_PROCESS_START.MONDAY                  1    0.3579 1739.4 -64518
- ORGANIZATION_TYPE.Industry..type.13                1    0.3803 1739.4 -64518
- ORGANIZATION_TYPE.Transport..type.3                1    0.4040 1739.5 -64518
- AMT_REQ_CREDIT_BUREAU_WEEK                         1    0.4042 1739.5 -64518
- NAME_INCOME_TYPE.State.servant                     1    0.4368 1739.5 -64517
- NAME_TYPE_SUITE.Family                             1    0.4387 1739.5 -64517
- OCCUPATION_TYPE.Cooking.staff                      1    0.4624 1739.5 -64517
- WEEKDAY_APPR_PROCESS_START.SUNDAY                  1    0.4753 1739.5 -64517
- ORGANIZATION_TYPE.Realtor                          1    0.5320 1739.6 -64516
- OCCUPATION_TYPE.High.skill.tech.staff              1    0.6525 1739.7 -64514
- REGION_RATING_CLIENT                               1    0.7530 1739.8 -64513
- NAME_CONTRACT_TYPE.Revolving.loans                 1    0.7700 1739.8 -64513
- DAYS_ID_PUBLISH                                    1    0.7701 1739.8 -64513
- OCCUPATION_TYPE.Low.skill.Laborers                 1    0.8017 1739.8 -64512
- ORGANIZATION_TYPE.Self.employed                    1    0.8642 1739.9 -64511
- ORGANIZATION_TYPE.Construction                     1    0.8981 1740.0 -64511
- ORGANIZATION_TYPE.Business.Entity.Type.3           1    1.0409 1740.1 -64509
- AMT_ANNUITY                                        1    1.1490 1740.2 -64507
- DAYS_EMPLOYED                                      1    1.2949 1740.3 -64505
- DAYS_BIRTH                                         1    1.3641 1740.4 -64504
- FLAG_WORK_PHONE                                    1    1.7107 1740.8 -64499
- AMT_CREDIT                                         1    2.7980 1741.8 -64484
- NAME_EDUCATION_TYPE.Secondary...secondary.special  1    2.9545 1742.0 -64482
- FLAG_OWN_CAR.Y                                     1    2.9712 1742.0 -64482
- AMT_GOODS_PRICE                                    1    3.3587 1742.4 -64476
- CODE_GENDER.M                                      1    3.8714 1742.9 -64469
- EXT_SOURCE_2                                       1   25.4149 1764.5 -64169
- EXT_SOURCE_3                                       1   30.0509 1769.1 -64105

```{r, eval=FALSE, echo=TRUE}
## Backward Stepwise Regression
#####

bck_names <- names(backward.lm$coefficients)
bck_loop <- "TARGET ~ "

for (name in bck_names[2: length(bck_names)]) {
  bck_loop <- paste0(bck_loop, "+", name, sep = "")
}

bck_all <- as.formula(bck_loop)

bck <- lm(bck_all, data = dummy_train)
```

Compute training and test MSE

```{r, eval=FALSE, echo=TRUE}
# Compute training MSE
yhat_bck_train <- predict(bck)
mse_bck_train <- mean((dummy_train$TARGET- yhat_bck_train)^2)

# Compute test MSE
yhat_bck_test <- predict(bck, dummy_test)
mse_bck_test <- mean((dummy_test$TARGET- yhat_bck_test)^2)

print(paste("MSE of training dataset is", signif(mse_bck_train,4 )))
print(paste("MSE of testing dataset is", signif(mse_bck_test,4 )))
```

```{r}
mse_bck_train = 0.06985
mse_bck_test = 0.06987

print(paste("MSE of training dataset is", signif(mse_bck_train,4 )))
print(paste("MSE of testing dataset is", signif(mse_bck_test,4 )))
```


### Decision Tree
Fisrt we generate a big raw decision tree:
```{r}
f1 <- as.formula(TARGET ~ . -SK_ID_CURR -TRAIN)

fit_tree <- rpart(f1, dummy_subset_train,
                  control = rpart.control(cp = 0.001))

rpart.plot(fit_tree, type = 3, main="Raw Decision Tree")

```

Then, prune the tree by 1-SE method where cp=0.0088617
```{r}
pruned_tree <- prune(fit_tree, cp=0.0088617)
rpart.plot(pruned_tree, main="Pruned Decision Tree")
```

That seems external sources dominate all predictors. Let's try to exclude the external sources and build a new decision tree model:

```{r}
f2 <- as.formula(TARGET ~ . -SK_ID_CURR -TRAIN - EXT_SOURCE_2 - EXT_SOURCE_3)

fit_tree_noext <- rpart(f1, dummy_subset_train[, -c(137, 138)],
                  control = rpart.control(cp = 0.001))
pruned_tree_noext <- prune(fit_tree_noext, cp=0.0051282)
rpart.plot(pruned_tree_noext, main="Pruned Decision Tree")
```
From the plot above, we can see `DAYS_BIRTH` which is applicant's age and `NAME_EDUCATION_TYPE` is the predictors effect the default rate most.


```{r}
yhat_train_tree <- predict(fit_tree, dummy_train)
mse_train_tree <- mean((dummy_train$TARGET - yhat_train_tree)^2)

yhat_test_tree <- predict(fit_tree, dummy_test)
mse_test_tree <- mean((dummy_test$TARGET - yhat_test_tree)^2)

print(paste("MSE of training dataset is", signif(mse_train_tree,4 )))
print(paste("MSE of testing dataset is", signif(mse_test_tree,4 )))
```

### Random Forest
```{r}
fit_rf <- randomForest(f1, dummy_subset_train, ntree = 500, do.trace = F)
## Check which variables are most predictive using a variable importance plot.
varImpPlot(fit_rf)

## Predictions and compute a train MSE.
yhat_rf_train <- predict(fit_rf, dummy_train) 
mse_rf_train <- mean((yhat_rf_train - dummy_train$TARGET) ^ 2)
print(mse_rf_train)

## Predictions and compute the MSE's.
yhat_rf_test <- predict(fit_rf, dummy_test)
mse_rf_test <- mean((yhat_rf_test - dummy_test$TARGET) ^ 2)
print(mse_rf_test)
```

### Boosting
Here we tried to optimize the model by tuning the parameters through K-fold cross validations, the best model would have lowest RMSE in validation dataset.   
In order to save time, we only choose to tune the `interaction.depth` parameters, set other parameters in the function as constant. We also randomly selected $\frac{1}{10}$ `application_train` to be `subset_train`, and used it to find out the optimized model, then apply it to the complete dataset.   

```{r}
f_boosting <- as.formula(TARGET ~ . - SK_ID_CURR - TRAIN)
```


```{r, eval=FALSE, echo=TRUE}
#Because it was extremely time-comsuming to train the model with such large sample size, so we decided not to run it again when knitted the outcome document.

fitControl <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated five times
                           repeats = 5)

gbmGrid <-  expand.grid(interaction.depth = 1:5, 
                        n.trees = 200, 
                        shrinkage = 0.01,
                        n.minobsinnode = 10)

set.seed(7)
gbmFit <- train(f_boosting, data = subset_train, 
                 method = "gbm", 
                 trControl = fitControl,
                 verbose = FALSE,
                tuneGrid = gbmGrid)

gbmFit
```

As the result, best performed model has `interactin.depth = 4`.  
Then we applied it on the complete `application_train` dataset.

```{r}
fit_btree <- gbm(f2,
data = application_train,
distribution = "gaussian",
n.trees = 500,
interaction.depth = 4,
shrinkage = 0.01)
```


```{r}
relative.influence(fit_btree)

yhat_btree <- predict(fit_btree, application_train, n.trees = 200)
mse_btree <- mean((yhat_btree - application_train$TARGET) ^ 2)

yhat_btree_test <- predict(fit_btree, application_test, n.trees = 200)
mse_btree_test <- mean((yhat_btree_test - application_test$TARGET) ^ 2)

print(paste("MSE of training dataset is", signif(mse_btree,4 )))
print(paste("MSE of testing dataset is", signif(mse_btree_test,4 )))
```


```{r}
mse_result <- tibble(Model = c("Linear Regression", "Forward Selection", "Backward Selection",
                               "Ridge", "Lasso", "Decision Trees",
                               "Random Forest", "Boosting Trees"), 
                     MSE_Train= c(signif(0.06972772,6), signif(0.06986146,6), signif(0.06985078,6),
                                  signif(0.06976723,6), signif(0.06973808,6), signif(0.07149059,6),
                                  signif(0.06535254,6), signif(0.07020926,6)),
                     MSE_Test = c(signif(0.06982647,6), signif(0.06988122,6), signif(0.06987248,6),
                                  signif(0.06986277,6), signif(0.06982388,6), signif(0.07192651,6),
                                  signif(0.07106714,6), signif(0.070292,6)))
mse_tidy <- gather(mse_result, type, mse, -Model)
```



```{r}
ggplot(mse_tidy, aes(x=Model, y=mse, fill=type)) +
  geom_histogram(stat = "identity", position = "dodge") +
   geom_hline(yintercept = 0.06982388, linetype="dashed") +
  coord_cartesian(ylim = c(0.065, 0.072)) +
  theme(axis.text.x = element_text(angle = 50, vjust = 0.65))
```


